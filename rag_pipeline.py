from typing import List, Dict, Any

import streamlit as st
#from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.vectorstores import VectorStore
from langchain_community.vectorstores import Pinecone as PineconeLang
from pinecone import Pinecone
from langchain_core.documents import Document as LCDocument
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents.stuff import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain

from langchain_docling import DoclingLoader
from langchain_docling.loader import ExportType

from pdf2image import convert_from_path
import pytesseract
from PIL import Image

from pathlib import Path

from layout_ocr import layout_ocr_from_pdf

from utils import (
    sanitize_metadata,
    log_time,
    prefix_documents_for_e5,
    count_tokens,
    format_response,
)

# Configura√ß√µes
DATA_FOLDER = Path("data")
DOCUMENTS_FOLDER = DATA_FOLDER / "documentos"
INDEX_FOLDER = DATA_FOLDER / "indexes"
INDEX_FOLDER.mkdir(parents=True, exist_ok=True)
EMBEDDING_MODEL_NAME = "intfloat/multilingual-e5-large"
LLM_MODEL_NAME = "claude-sonnet-4-20250514"
TOKEN_LIMIT = 7000  # limite aproximado de tokens para prompt


@log_time
def load_documents_with_docling(
    file_path: str,
    export_type: ExportType = ExportType.DOC_CHUNKS
) -> List[LCDocument]:
    """
    1¬∞ - O DoclingLoader l√™ o PDF, extrai o texto estruturado em ‚Äúchunks‚Äù
         e retorna uma lista de Documentos.
    """
    loader = DoclingLoader(file_path=file_path, export_type=export_type)
    return loader.load()

# @log_time
# def load_documents_with_ocr(file_path: str) -> List[LCDocument]:
#     """
#     Alternativa ao Docling: extrai texto via OCR com pytesseract de PDFs com p√°ginas digitalizadas (sem texto acess√≠vel).
#     """
#     try:
#         # Converte cada p√°gina do PDF em uma imagem
#         images = convert_from_path(file_path, dpi=300)
#         if not images:
#             st.warning("‚ö†Ô∏è Nenhuma p√°gina foi detectada no PDF.")
#             return []
#         documents = []

#         for i, image in enumerate(images):
#             try:
#                 text = pytesseract.image_to_string(image, lang="por")
#             except:
#                 text = pytesseract.image_to_string(image)  
#             if text.strip():
#                 documents.append(LCDocument(page_content=text, metadata={"page": i + 1}))
        
#         if not documents:
#             st.warning("‚ö†Ô∏è OCR n√£o conseguiu extrair texto. Verifique a qualidade do PDF.")
#         return documents

#     except Exception as e:
#         st.error(f"Erro durante OCR com pytesseract: {e}")
#         return []

# def split_text_into_chunks(documents: List[LCDocument]) -> List[LCDocument]:
#     """
#     2¬∞ - O RecursiveCharacterTextSplitter divide os documentos em peda√ßos
#          menores (300 tokens com 100 de sobreposi√ß√£o) para otimizar embeddings.
#     """
#     splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=100)
#     return splitter.split_documents(documents)


@log_time
def create_or_load_vectorstore(
    file_path: str,
    documents: List[LCDocument],
    embeddings: HuggingFaceEmbeddings
):# -> FAISS:
    """
    Substitui o FAISS por Pinecone.

    3¬∞ - Cria ou carrega um √≠ndice FAISS:
         - Gera um hash a partir do nome do arquivo para nomear a pasta de √≠ndice.
         - Se j√° existir, carrega; caso contr√°rio, cria e salva o novo √≠ndice.
    """
    try:
        # Inicializa cliente Pinecone
        pc = Pinecone(api_key=st.secrets["PINECONE_API_KEY"])

        index_name = "legalmentor"

        # Confere se o index existe
        if index_name not in pc.list_indexes().names():
            st.error(f"Index '{index_name}' n√£o existe no Pinecone.")
            return None

        for doc in documents:
            doc.metadata = sanitize_metadata(doc.metadata)

        # Cria vectorstore com LangChain + Pinecone
        vectorstore = PineconeLang.from_documents(
            documents=documents,
            embedding=embeddings,
            index_name=index_name,
            namespace="default"
        )

        return vectorstore

    except Exception as e:
        st.error(f"Erro ao conectar ao Pinecone: {e}")
        return None


def create_rag_chain(vectorstore: VectorStore) -> Any:
    """
    4¬∞ - Cria o retriever com MMR e configura o LLM ANTHROPIC + Claude,
         monta a cadeia de documentos e retrieval e retorna um wrapper.
    """
    retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": 20, "fetch_k": 100, "lambda_mult": 0.8},
    )

    llm = ChatAnthropic(
        temperature=0.1,
        model_name=LLM_MODEL_NAME,
        api_key=st.secrets["ANTHROPIC_API_KEY"],
        max_tokens=1000,  # Resposta gerada
    )
    
    # Prompt padr√£o com contexto e instru√ß√µes
    template = """
Voc√™ √© um assistente jur√≠dico especializado. Analise cuidadosamente o seguinte contexto extra√≠do de documentos jur√≠dicos e responda de forma objetiva, sem adicionar informa√ß√µes externas.

Se n√£o souber a resposta, diga "N√£o encontrei informa√ß√µes suficientes no documento."

<context>
{context}
</context>

Pergunta: {input}

Resposta:
"""
    prompt = ChatPromptTemplate.from_template(template)
    document_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)
    retrieval_chain = create_retrieval_chain(retriever, document_chain)

    class RagChainWrapper:
        """
        5¬∞ - Wrapper que:
             a) Estima tokens e alerta se pr√≥ximo do limite
             b) Invoca o chain original
             c) Exibe metadados e trechos do contexto recuperado
             d) Formata a resposta final
        """
        def __init__(self, chain):
            self._chain = chain

        def invoke(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
            # 1) Estima tokens do prompt
            approx = count_tokens(
                template.format(
                    context="",
                    input=inputs.get("input", "")
                ),
                model_name=EMBEDDING_MODEL_NAME
            )

            st.info(f"üìè Prompt estimado em ~{approx} tokens (limite configurado: {TOKEN_LIMIT}).")

            if approx > TOKEN_LIMIT:
                st.warning("Aten√ß√£o: o prompt est√° acima do limite seguro e pode causar erro.")

            # 2) Invoca o chain original
            output = self._chain.invoke(inputs)

            # 3) Exibe metadados e trechos de contexto recuperado
            # if "context" in output:
            #     st.write("üîç Documentos recuperados:")
            #     for doc in output["context"]:
            #         #st.write(f"{extract_metadata(doc)} ‚Äî {doc.page_content[:200]}‚Ä¶")
            #         st.write(f"{extract_metadata(doc)} ‚Äî {doc.page_content}")

            # 4) P√≥s‚Äëprocessa a resposta
            if "answer" in output:
                output["answer"] = format_response(output["answer"])

            return output

        __call__ = invoke # permite usar o wrapper como fun√ß√£o

    return RagChainWrapper(retrieval_chain)


@log_time
def process_document(file_path: str = None) -> Any:
    """
    6¬∞ - Pipeline completo:
         1) Carrega e prefixa documentos
         2) (Opcional) Chunk splitting
         3) Cria/recupera vectorstore Pinecone
         4) Constr√≥i e retorna a cadeia RAG personalizada
    """
    # 1) Load & prefix
    if file_path:
        docs = load_documents_with_docling(file_path)
        if not docs or not isinstance(docs, list) or all(not doc.page_content.strip() for doc in docs):
            st.warning("üìÑ O Docling n√£o encontrou texto acess√≠vel no PDF. Usando OCR avan√ßado com LayoutLM como fallback.")
            docs = layout_ocr_from_pdf(file_path)

        docs = prefix_documents_for_e5(docs)
    else:
        docs = []  # apenas inicializa o vectorstore vazio, pois j√° temos documentos indexados


    # 2) (opcional) split: use este passo somente se
    #    ‚Äì voc√™ estiver carregando o documento como um √∫nico bloco de texto (por ex. via PdfPlumber / PyPDF2),
    #    ‚Äì ou se quiser for√ßar chunks de tamanho fixo diferentes dos produzidos pelo Docling;
    #    Caso contr√°rio, o DoclingLoader com export_type=DOC_CHUNKS j√° retorna trechos bem estruturados
    #    (t√≠tulos, par√°grafos, tabelas) e n√£o vale a pena requebr√°-los com RecursiveCharacterTextSplitter.
    #
    # Para ativar, basta descomentar a linha abaixo:
    # docs = split_text_into_chunks(docs)

    # 3) embeddings e vectorstore
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
    vs = create_or_load_vectorstore(file_path or "default", docs, embeddings)

    # 4) Cria e retorna o wrapper da chain
    return create_rag_chain(vs)
